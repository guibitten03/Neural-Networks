{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standart MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pesos inicializados com distribuição gaussiana de media 0 e variança 1\n",
    "class MLP():\n",
    "    def __init__(self, in_Length, hid_Length, out_Length, l_rate=0.002):\n",
    "\n",
    "        self.W_in, self.B_in = self.initialize_weights(in_Length, hid_Length)\n",
    "        self.W_out, self.B_out = self.initialize_weights(hid_Length, out_Length)\n",
    "\n",
    "        self.l_rate = l_rate\n",
    "\n",
    "        \n",
    "    def initialize_weights(self, in_Length, hid_Length):\n",
    "        w = np.random.random((hid_Length, in_Length))\n",
    "        b = np.full((hid_Length), -1)\n",
    "\n",
    "        return w, b\n",
    "\n",
    "    def tanhFunc(self, x, deriv):\n",
    "        if deriv == True:\n",
    "            return (1.0 - (np.tanh(x)))\n",
    "        \n",
    "        return  np.tanh(x)\n",
    "    \n",
    "    def sigmoidFunc(self, x, deriv):\n",
    "        if deriv == True:\n",
    "            return x - np.power(x, 2)\n",
    "        \n",
    "        return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # O bias é adicionado após a ponderação e o dot product do vetor\n",
    "        # self.out_hidden = np.dot(x.T, self.W_in) - self.B_in\n",
    "        self.out_hidden = np.dot(x.T, self.W_in) - self.B_in\n",
    "        self.out_hidden_activate = self.sigmoidFunc(self.out_hidden, deriv=False)\n",
    "\n",
    "        self.output = np.dot(self.out_hidden_activate, self.W_out.T) - self.B_out\n",
    "        y = self.sigmoidFunc(self.output, deriv=False)\n",
    "\n",
    "        return y \n",
    "    \n",
    "    def backward(self, target, x):\n",
    "\n",
    "        # Gradient applied in last layer\n",
    "        y = self.forward(x)\n",
    "        error = (math.pow((target - y), 2)) / 2\n",
    "        local_gradient = error * self.sigmoidFunc(self.output, deriv=True)\n",
    "        self.W_out = self.W_out + (self.l_rate * local_gradient * self.out_hidden_activate)\n",
    "\n",
    "        # Gradient applied in first layer\n",
    "        local_gradient = -(np.dot(local_gradient, self.W_out) * self.sigmoidFunc(self.out_hidden, deriv=True))\n",
    "        self.W_in = self.W_in + (self.l_rate * local_gradient * x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(3, 3, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../datasets/Apendice_3/train.csv\")\n",
    "test_data = pd.read_csv(\"../datasets/Apendice_3/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84121304]\n",
      "[0.83720787]\n",
      "[0.836988]\n",
      "[0.83654097]\n",
      "[0.83522392]\n",
      "[0.83737223]\n",
      "[0.83681037]\n",
      "[0.83188905]\n",
      "[0.83151456]\n",
      "[0.82870995]\n",
      "[0.83279756]\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_data, n_epochs, error_limit):\n",
    "    batch_size = len(train_data) / 10\n",
    "\n",
    "    train_x = train_data.values\n",
    "\n",
    "    batches = np.array_split(train_x, batch_size)\n",
    "\n",
    "    error_list = []\n",
    "    for batch in batches:\n",
    "        preds = []\n",
    "        for row in batch:\n",
    "            input = row[:-1]\n",
    "            target = row[-1]\n",
    "\n",
    "            y_pred = model.backward(target, input)\n",
    "            print(y_pred)\n",
    "            preds.append((target - y_pred[0]))\n",
    "\n",
    "            if (len(error_list) != 0):\n",
    "                if (error_list[-1] - np.mean(np.array(preds))) < error_limit:\n",
    "                    return\n",
    "        \n",
    "        error_list.append(np.mean(np.array(preds)))\n",
    "\n",
    "    return error_list\n",
    "\n",
    "error = train(mlp, train_data, 10, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3875013070.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def test(model, )\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_data):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
