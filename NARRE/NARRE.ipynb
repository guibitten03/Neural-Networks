{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class NARRE(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, review_maxlen, user_num, item_num, latent_dim, hidden_dims):\n",
    "        super(NARRE, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.review_encoder = nn.LSTM(embedding_dim, latent_dim, num_layers=1, batch_first=True)\n",
    "        self.user_embedding = nn.Embedding(user_num, latent_dim)\n",
    "        self.item_embedding = nn.Embedding(item_num, latent_dim)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dims),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dims, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.rating_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dims),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dims, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, reviews, users, items):\n",
    "        embedded_reviews = self.embedding(reviews)\n",
    "        review_outputs, _ = self.review_encoder(embedded_reviews)\n",
    "        user_embedded = self.user_embedding(users)\n",
    "        item_embedded = self.item_embedding(items)\n",
    "        attention_scores = self.attention(review_outputs).squeeze()\n",
    "        attention_outputs = torch.bmm(attention_scores.unsqueeze(1), review_outputs).squeeze()\n",
    "        latent_features = torch.cat((attention_outputs, user_embedded, item_embedded), dim=1)\n",
    "        predicted_ratings = self.rating_predictor(latent_features)\n",
    "        return predicted_ratings\n",
    "\n",
    "# Exemplo de uso\n",
    "vocab_size = 10000  # Tamanho do vocabulário (número de palavras únicas)\n",
    "embedding_dim = 100  # Dimensão dos vetores de embedding\n",
    "review_maxlen = 200  # Tamanho máximo das avaliações\n",
    "user_num = 1000  # Número de usuários\n",
    "item_num = 2000  # Número de itens\n",
    "latent_dim = 50  # Dimensão dos vetores latentes\n",
    "hidden_dims = 100  # Dimensão da camada oculta\n",
    "\n",
    "# Criando uma instância do modelo\n",
    "model = NARRE(vocab_size, embedding_dim, review_maxlen, user_num, item_num, latent_dim, hidden_dims)\n",
    "\n",
    "# Definindo a função de perda e otimizador\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Exemplo de dados de entrada\n",
    "reviews = torch.from_numpy(np.random.randint(0, vocab_size, (32, review_maxlen))).long()\n",
    "users = torch.from_numpy(np.random.randint(0, user_num, (32,))).long()\n",
    "items = torch.from_numpy(np.random.randint(0, item_num, (32,))).long()\n",
    "ratings = torch.from_numpy(np.random.randint(1, 6, (32, 1))).float()\n",
    "\n",
    "# Loop de treinamento\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predicted_ratings = model(reviews, users, items)\n",
    "    loss = loss_fn(predicted_ratings, ratings)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NARRE(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim, num_reviews):\n",
    "        super(NARRE, self).__init__()\n",
    "\n",
    "        self.user_embed = nn.Embedding(num_users, embed_dim)\n",
    "        self.item_embed = nn.Embedding(num_items, embed_dim)\n",
    "        self.review_embed = nn.EmbeddingBag(num_reviews, embed_dim)\n",
    "\n",
    "        self.attention = nn.Linear(embed_dim, 1)\n",
    "        self.prediction = nn.Linear(embed_dim * 2, 1)\n",
    "\n",
    "    def forward(self, user_ids, item_ids, review_offsets):\n",
    "        user_embeds = self.user_embed(user_ids)\n",
    "        item_embeds = self.item_embed(item_ids)\n",
    "        review_embeds = self.review_embed(review_offsets)\n",
    "\n",
    "        # Calculate review attentions\n",
    "        review_attentions = F.softmax(self.attention(review_embeds), dim=0)\n",
    "        review_sum = torch.sum(review_embeds * review_attentions, dim=0)\n",
    "\n",
    "        # Concatenate user and item embeddings\n",
    "        user_item_embeds = torch.cat([user_embeds, item_embeds], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        preds = self.prediction(user_item_embeds * review_sum)\n",
    "\n",
    "        return preds.squeeze()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
